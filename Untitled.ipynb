{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920be524-d222-4f80-b2de-719ca0d623c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for chart to load...\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff7770788e5\n\t0x7ff777078940\n\t0x7ff776e5165d\n\t0x7ff776ea9a33\n\t0x7ff776ea9d3c\n\t0x7ff776efdf67\n\t0x7ff776efac97\n\t0x7ff776e9ac29\n\t0x7ff776e9ba93\n\t0x7ff777390640\n\t0x7ff77738af80\n\t0x7ff7773a96e6\n\t0x7ff777095de4\n\t0x7ff77709ed8c\n\t0x7ff777082004\n\t0x7ff7770821b5\n\t0x7ff777067ee2\n\t0x7ffbd2c0259d\n\t0x7ffbd34caf78\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 3. WAIT: Allow time for JavaScript to render the chart\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# We wait specifically for the 'scattergeo' class shown in your screenshot\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for chart to load...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscattergeo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Optional: small sleep to ensure data binding is complete\u001b[39;00m\n\u001b[0;32m     23\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\ergo\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:138\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff7770788e5\n\t0x7ff777078940\n\t0x7ff776e5165d\n\t0x7ff776ea9a33\n\t0x7ff776ea9d3c\n\t0x7ff776efdf67\n\t0x7ff776efac97\n\t0x7ff776e9ac29\n\t0x7ff776e9ba93\n\t0x7ff777390640\n\t0x7ff77738af80\n\t0x7ff7773a96e6\n\t0x7ff777095de4\n\t0x7ff77709ed8c\n\t0x7ff777082004\n\t0x7ff7770821b5\n\t0x7ff777067ee2\n\t0x7ffbd2c0259d\n\t0x7ffbd34caf78\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# 1. SETUP: Configure the URL and Browser\n",
    "url = \"https://ajguk-my.sharepoint.com/:u:/g/personal/francesco_zovi_gallagherre_com/IQCIJ1mzZEu4TKuxkowIoFyKAV4-ambuXM3UFTEiEtqNwbQ?e=1kyerG\"  # Replace with the actual URL\n",
    "driver = webdriver.Chrome()      # Or webdriver.Firefox(), etc.\n",
    "\n",
    "try:\n",
    "    # 2. LOAD PAGE\n",
    "    driver.get(url)\n",
    "    \n",
    "    # 3. WAIT: Allow time for JavaScript to render the chart\n",
    "    # We wait specifically for the 'scattergeo' class shown in your screenshot\n",
    "    print(\"Waiting for chart to load...\")\n",
    "    WebDriverWait(driver, 40).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"scattergeo\"))\n",
    "    )\n",
    "    # Optional: small sleep to ensure data binding is complete\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 4. EXTRACT DATA via JavaScript\n",
    "    # Plotly graphs usually live in a div with class 'js-plotly-plot' \n",
    "    # and have a .data property attached to the DOM element.\n",
    "    script = \"\"\"\n",
    "    var graph = document.getElementsByClassName('js-plotly-plot')[0];\n",
    "    if (graph && graph.data) {\n",
    "        return graph.data;\n",
    "    } else {\n",
    "        return null;\n",
    "    }\n",
    "    \"\"\"\n",
    "    chart_data = driver.execute_script(script)\n",
    "\n",
    "    if not chart_data:\n",
    "        print(\"Could not find Plotly data object. The chart might not be standard Plotly.\")\n",
    "    else:\n",
    "        # 5. PROCESS DATA\n",
    "        # chart_data is a list of 'traces' (groups shown in the legend)\n",
    "        # Each trace contains lists of lats, lons, and text.\n",
    "        \n",
    "        all_points = []\n",
    "        \n",
    "        for trace in chart_data:\n",
    "            # Get the group name (e.g., \"Poliambulatori\", \"RSA\")\n",
    "            group_name = trace.get('name', 'Unknown')\n",
    "            \n",
    "            # Extract coordinates and text labels\n",
    "            lats = trace.get('lat', [])\n",
    "            lons = trace.get('lon', [])\n",
    "            texts = trace.get('text', []) # Sometimes calls 'hovertext'\n",
    "            \n",
    "            # If text is empty, check hovertext\n",
    "            if not texts:\n",
    "                texts = trace.get('hovertext', [])\n",
    "            \n",
    "            # If still empty, fill with blanks\n",
    "            if not texts:\n",
    "                texts = [\"\"] * len(lats)\n",
    "\n",
    "            # Zip them together into rows\n",
    "            for lat, lon, txt in zip(lats, lons, texts):\n",
    "                all_points.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Latitude\": lat,\n",
    "                    \"Longitude\": lon,\n",
    "                    \"Description\": txt\n",
    "                })\n",
    "\n",
    "        # 6. SAVE TO CSV\n",
    "        if all_points:\n",
    "            df = pd.DataFrame(all_points)\n",
    "            df.to_csv(\"map_data.csv\", index=False)\n",
    "            print(f\"Successfully scraped {len(df)} locations to 'map_data.csv'.\")\n",
    "            print(df.head())\n",
    "        else:\n",
    "            print(\"Found the chart object, but it contained no coordinate data.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6d6d51-8b4b-45ca-8bb4-9ae93427a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://ajguk-my.sharepoint.com/:u:/g/personal/francesco_zovi_gallagherre_com/IQCIJ1mzZEu4TKuxkowIoFyKAV4-ambuXM3UFTEiEtqNwbQ?e=1kyerG...\n",
      "Waiting 10 seconds for map to render...\n",
      "Screenshot saved to 'debug_view.png'. Please check if the map is visible in this image.\n",
      "Starting recursive search for Plotly data...\n",
      "FAILED: Could not find Plotly data in any frame.\n",
      "Check 'debug_view.png'. If the map is there, it might be using Shadow DOM or Canvas.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 1. SETUP\n",
    "url = \"https://ajguk-my.sharepoint.com/:u:/g/personal/francesco_zovi_gallagherre_com/IQCIJ1mzZEu4TKuxkowIoFyKAV4-ambuXM3UFTEiEtqNwbQ?e=1kyerG\"  # Replace with the actual URL\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\") # Keep this commented out so you can see it working\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window() # Crucial: Ensure map renders\n",
    "\n",
    "def search_frames_for_plotly(driver, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively searches current frame and all child iframes for Plotly data.\n",
    "    \"\"\"\n",
    "    # Attempt to extract data from the current frame using JS\n",
    "    # We look for the standard Plotly class OR the global Plotly object registry\n",
    "    script = \"\"\"\n",
    "    // Strategy A: Look for the DOM element\n",
    "    var graph = document.getElementsByClassName('js-plotly-plot')[0];\n",
    "    if (graph && graph.data) return graph.data;\n",
    "\n",
    "    // Strategy B: Look for the internal Plotly registry (often works if class names change)\n",
    "    if (window.Plotly && window.Plotly.d3) {\n",
    "        var plots = window.Plotly.d3.selectAll('.js-plotly-plot').data();\n",
    "        if (plots.length > 0 && plots[0].length > 0) return plots[0];\n",
    "    }\n",
    "    \n",
    "    return null;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = driver.execute_script(script)\n",
    "        if data:\n",
    "            print(f\"Found data at depth {depth}!\")\n",
    "            return data\n",
    "    except:\n",
    "        pass # Ignore JS errors in specific frames\n",
    "\n",
    "    # If not found, look for child iframes and recurse\n",
    "    iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "    \n",
    "    for i, frame in enumerate(iframes):\n",
    "        # Switch to frame\n",
    "        try:\n",
    "            driver.switch_to.frame(frame)\n",
    "            # Recurse\n",
    "            result = search_frames_for_plotly(driver, depth + 1)\n",
    "            if result:\n",
    "                return result\n",
    "            # Switch back to parent to continue loop\n",
    "            driver.switch_to.parent_frame()\n",
    "        except:\n",
    "            # If a frame is locked (cross-origin) or closes, skip it\n",
    "            driver.switch_to.parent_frame()\n",
    "            continue\n",
    "            \n",
    "    return None\n",
    "\n",
    "try:\n",
    "    print(f\"Loading {url}...\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Give it plenty of time to render everything\n",
    "    print(\"Waiting 10 seconds for map to render...\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Take a screenshot for debugging\n",
    "    driver.save_screenshot(\"debug_view.png\")\n",
    "    print(\"Screenshot saved to 'debug_view.png'. Please check if the map is visible in this image.\")\n",
    "\n",
    "    # Start the hunt\n",
    "    print(\"Starting recursive search for Plotly data...\")\n",
    "    chart_data = search_frames_for_plotly(driver)\n",
    "\n",
    "    if not chart_data:\n",
    "        print(\"FAILED: Could not find Plotly data in any frame.\")\n",
    "        print(\"Check 'debug_view.png'. If the map is there, it might be using Shadow DOM or Canvas.\")\n",
    "    else:\n",
    "        print(\"Data object found! Processing...\")\n",
    "        \n",
    "        all_points = []\n",
    "        for trace in chart_data:\n",
    "            # Check if this trace has coordinate data\n",
    "            if 'lat' in trace and 'lon' in trace:\n",
    "                name = trace.get('name', 'Unknown')\n",
    "                lats = trace.get('lat', [])\n",
    "                lons = trace.get('lon', [])\n",
    "                \n",
    "                # Normalize text fields\n",
    "                texts = trace.get('text', [])\n",
    "                if not texts: texts = trace.get('hovertext', [])\n",
    "                \n",
    "                # Ensure text is a list of the correct length\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts] * len(lats)\n",
    "                elif not texts:\n",
    "                    texts = [\"\"] * len(lats)\n",
    "\n",
    "                for lat, lon, txt in zip(lats, lons, texts):\n",
    "                    all_points.append({\n",
    "                        \"Group\": name,\n",
    "                        \"Latitude\": lat,\n",
    "                        \"Longitude\": lon,\n",
    "                        \"Description\": txt\n",
    "                    })\n",
    "\n",
    "        if len(all_points) > 0:\n",
    "            df = pd.DataFrame(all_points)\n",
    "            df.to_csv(\"earthquake_map_data.csv\", index=False)\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"SUCCESS: Saved {len(df)} rows to 'earthquake_map_data.csv'\")\n",
    "            print(\"-\" * 30)\n",
    "            print(df.head())\n",
    "        else:\n",
    "            print(\"Found the chart object, but it contained no Lat/Lon data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beac8b76-7fff-4eea-ad3b-e8576d0dd5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\rajoshi\\Downloads\\1_grafico_interattivo_regioni_reali.html...\n",
      "File loaded: 7709490 characters.\n",
      "Searching for JavaScript data pattern...\n",
      "Found 428 trace(s). Processing...\n",
      "========================================\n",
      "SUCCESS! Scraped 74940 locations.\n",
      "Saved to: extracted_map_data.csv\n",
      "========================================\n",
      "                            Group Latitude Longitude  \\\n",
      "0                 Poliambulatori     dtype     dtype   \n",
      "1                 Poliambulatori     bdata     bdata   \n",
      "2             Studi Odontoiatrici    dtype     dtype   \n",
      "3             Studi Odontoiatrici    bdata     bdata   \n",
      "4  Poliambulatorio con chirurgia     dtype     dtype   \n",
      "\n",
      "                      Description  \n",
      "0                 Poliambulatori   \n",
      "1                 Poliambulatori   \n",
      "2             Studi Odontoiatrici  \n",
      "3             Studi Odontoiatrici  \n",
      "4  Poliambulatorio con chirurgia   \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Replace this with the path to the file you downloaded\n",
    "file_path = r\"C:\\Users\\rajoshi\\Downloads\\1_grafico_interattivo_regioni_reali.html\" \n",
    "# ==========================================\n",
    "\n",
    "def extract_plotly_data_from_html(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback if utf-8 fails\n",
    "        with open(filepath, 'r', encoding='latin-1') as f:\n",
    "            html_content = f.read()\n",
    "\n",
    "    print(f\"File loaded: {len(html_content)} characters.\")\n",
    "\n",
    "    # METHOD 1: Look for data inside <script type=\"application/json\">\n",
    "    # This is common in R/HTMLWidgets/Folium exports\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    json_scripts = soup.find_all('script', type='application/json')\n",
    "    \n",
    "    for script in json_scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            # Sometimes the data is wrapped in an 'x' object\n",
    "            if 'x' in data and 'data' in data['x']:\n",
    "                print(\"Found data in JSON script tag!\")\n",
    "                return data['x']['data']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # METHOD 2: Look for raw JavaScript \"Plotly.newPlot\" call\n",
    "    # This matches your snippet style\n",
    "    print(\"Searching for JavaScript data pattern...\")\n",
    "    \n",
    "    # Regex explanation:\n",
    "    # 1. Look for 'Plotly.newPlot('\n",
    "    # 2. Skip the div ID (first argument)\n",
    "    # 3. Capture everything inside the second argument [...] (the data)\n",
    "    # 4. Stop when we hit the comma before the layout object {,\n",
    "    pattern = r\"Plotly\\.newPlot\\(\\s*['\\\"].*?['\\\"]\\s*,\\s*(\\[[\\s\\S]*?\\])\\s*,\\s*\\{\"\n",
    "    \n",
    "    match = re.search(pattern, html_content)\n",
    "    \n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        try:\n",
    "            # Clean up potential JavaScript weirdness to make it valid JSON\n",
    "            # Sometimes JS uses ' instead of \" which JSON hates\n",
    "            # This is a basic cleanup, might not catch everything\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Found the data block, but it contains raw JavaScript that isn't valid JSON.\")\n",
    "            print(\"Attempting to fix quotes...\")\n",
    "            try:\n",
    "                # specific fix for JS objects using keys without quotes\n",
    "                # This is risky but often works for simple Plotly dumps\n",
    "                return json.loads(json_str.replace(\"'\", '\"'))\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "print(f\"Reading {file_path}...\")\n",
    "chart_data = extract_plotly_data_from_html(file_path)\n",
    "\n",
    "if not chart_data:\n",
    "    print(\"Could not extract data. The file format might be unique.\")\n",
    "else:\n",
    "    all_points = []\n",
    "    print(f\"Found {len(chart_data)} trace(s). Processing...\")\n",
    "\n",
    "    for trace in chart_data:\n",
    "        # Check for lat/lon keys\n",
    "        if 'lat' in trace and 'lon' in trace:\n",
    "            # Name of the group (e.g., Poliambulatori)\n",
    "            group_name = trace.get('name', 'Unknown')\n",
    "            \n",
    "            lats = trace.get('lat', [])\n",
    "            lons = trace.get('lon', [])\n",
    "            \n",
    "            # Text extraction logic\n",
    "            texts = trace.get('text', [])\n",
    "            if not texts: \n",
    "                texts = trace.get('hovertext', [])\n",
    "            \n",
    "            # Normalize list lengths\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts] * len(lats)\n",
    "            elif not texts:\n",
    "                texts = [\"\"] * len(lats)\n",
    "\n",
    "            for lat, lon, txt in zip(lats, lons, texts):\n",
    "                all_points.append({\n",
    "                    \"Group\": group_name,\n",
    "                    \"Latitude\": lat,\n",
    "                    \"Longitude\": lon,\n",
    "                    \"Description\": txt\n",
    "                })\n",
    "\n",
    "    if all_points:\n",
    "        df = pd.DataFrame(all_points)\n",
    "        output_filename = \"extracted_map_data.csv\"\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(\"=\"*40)\n",
    "        print(f\"SUCCESS! Scraped {len(df)} locations.\")\n",
    "        print(f\"Saved to: {output_filename}\")\n",
    "        print(\"=\"*40)\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"Found Plotly structure, but no geographic data (lat/lon) inside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c935fa-4bc1-46b6-849d-2203ff2720c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\rajoshi\\Downloads\\1_grafico_interattivo_regioni_reali.html...\n",
      "Found 428 layers. decoding...\n",
      "========================================\n",
      "SUCCESS! Extracted 76092 locations.\n",
      "========================================\n",
      "             Group   Latitude  Longitude  Magnitude_or_Size      Description\n",
      "0  Poliambulatori   41.905781  12.887392          2165610.0  Poliambulatori \n",
      "1  Poliambulatori   41.999815  12.726347           159000.0  Poliambulatori \n",
      "2  Poliambulatori   42.051643  12.616739           127000.0  Poliambulatori \n",
      "3  Poliambulatori   42.067010  12.765451           690000.0  Poliambulatori \n",
      "4  Poliambulatori   41.860174  13.032261           100000.0  Poliambulatori \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "file_path = r\"C:\\Users\\rajoshi\\Downloads\\1_grafico_interattivo_regioni_reali.html\" \n",
    "# ==========================================\n",
    "\n",
    "def decode_bdata(data_obj):\n",
    "    \"\"\"\n",
    "    Decodes Plotly binary data (base64) into a standard Python list.\n",
    "    \"\"\"\n",
    "    # If it's just a normal list, return it as is\n",
    "    if isinstance(data_obj, list):\n",
    "        return data_obj\n",
    "    \n",
    "    # If it's a dictionary containing 'bdata', decode it\n",
    "    if isinstance(data_obj, dict) and 'bdata' in data_obj and 'dtype' in data_obj:\n",
    "        try:\n",
    "            decoded_bytes = base64.b64decode(data_obj['bdata'])\n",
    "            dtype = data_obj['dtype']\n",
    "            \n",
    "            # Convert binary to numpy array based on type\n",
    "            # Common types: 'float64', 'float32', 'int32', 'uint8'\n",
    "            return np.frombuffer(decoded_bytes, dtype=dtype).tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding binary block: {e}\")\n",
    "            return []\n",
    "            \n",
    "    return []\n",
    "\n",
    "def extract_and_decode(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "    except:\n",
    "        with open(filepath, 'r', encoding='latin-1') as f:\n",
    "            html = f.read()\n",
    "\n",
    "    # 1. Regex to find the JSON data block inside Plotly.newPlot(...)\n",
    "    # We look for the 2nd argument which is the data array\n",
    "    pattern = r\"Plotly\\.newPlot\\(\\s*['\\\"].*?['\\\"]\\s*,\\s*(\\[[\\s\\S]*?\\])\\s*,\\s*\\{\"\n",
    "    match = re.search(pattern, html)\n",
    "\n",
    "    if not match:\n",
    "        print(\"Could not find Plotly.newPlot call via Regex. Trying BS4/Script tags...\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        scripts = soup.find_all('script', type='application/json')\n",
    "        for s in scripts:\n",
    "            try:\n",
    "                js = json.loads(s.string)\n",
    "                if 'x' in js and 'data' in js['x']:\n",
    "                    return js['x']['data']\n",
    "            except: pass\n",
    "        return None\n",
    "\n",
    "    # 2. Parse the JSON string\n",
    "    json_str = match.group(1)\n",
    "    try:\n",
    "        data_raw = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Simple attempt to fix JS quotes\n",
    "        try:\n",
    "            data_raw = json.loads(json_str.replace(\"'\", '\"'))\n",
    "        except:\n",
    "            print(\"Failed to parse the extracted JSON string.\")\n",
    "            return None\n",
    "\n",
    "    # 3. Process the traces\n",
    "    cleaned_rows = []\n",
    "    \n",
    "    print(f\"Found {len(data_raw)} layers. decoding...\")\n",
    "    \n",
    "    for trace in data_raw:\n",
    "        # Check if this trace has coordinates (skip background maps/shapes)\n",
    "        if 'lat' not in trace or 'lon' not in trace:\n",
    "            continue\n",
    "            \n",
    "        # --- DECODING HAPPENS HERE ---\n",
    "        lats = decode_bdata(trace['lat'])\n",
    "        lons = decode_bdata(trace['lon'])\n",
    "        \n",
    "        # If decoding failed or empty, skip\n",
    "        if not lats or not lons:\n",
    "            continue\n",
    "\n",
    "        # Get Text/Labels\n",
    "        texts = trace.get('text', [])\n",
    "        if not texts: texts = trace.get('hovertext', [])\n",
    "        # Ensure text is a list\n",
    "        if isinstance(texts, str): texts = [texts] * len(lats)\n",
    "        if not texts: texts = [\"\"] * len(lats)\n",
    "\n",
    "        # --- EXTRACT MAGNITUDE ---\n",
    "        # Magnitude is usually in trace['marker']['size'] or trace['marker']['color']\n",
    "        mags = []\n",
    "        marker = trace.get('marker', {})\n",
    "        \n",
    "        # Try Size first\n",
    "        if 'size' in marker:\n",
    "            mags = decode_bdata(marker['size'])\n",
    "        # Try Color if size is missing/constant\n",
    "        elif 'color' in marker and isinstance(marker['color'], (list, dict)):\n",
    "            mags = decode_bdata(marker['color'])\n",
    "        \n",
    "        # If single value or missing, fill with N/A\n",
    "        if not isinstance(mags, list) or len(mags) != len(lats):\n",
    "            mags = [None] * len(lats)\n",
    "\n",
    "        group_name = trace.get('name', 'Unknown')\n",
    "\n",
    "        # Zip it all together\n",
    "        for lat, lon, txt, mag in zip(lats, lons, texts, mags):\n",
    "            cleaned_rows.append({\n",
    "                \"Group\": group_name,\n",
    "                \"Latitude\": lat,\n",
    "                \"Longitude\": lon,\n",
    "                \"Magnitude_or_Size\": mag,\n",
    "                \"Description\": txt\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(cleaned_rows)\n",
    "\n",
    "# ==========================================\n",
    "# RUN\n",
    "# ==========================================\n",
    "print(f\"Processing {file_path}...\")\n",
    "df = extract_and_decode(file_path)\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"=\"*40)\n",
    "    print(f\"SUCCESS! Extracted {len(df)} locations.\")\n",
    "    print(\"=\"*40)\n",
    "    print(df.head())\n",
    "    df.to_csv(\"final_earthquake_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"No data found. The file might not contain point data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
